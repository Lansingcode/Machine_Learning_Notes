### 算法层面  
1. 算法层面Xgboost加了正则项，普通GBDT没有  
正则项可以防止过拟合  
2. Xgboost损失函数是误差部分的二阶泰勒展开，GBDT是一阶泰勒展开，因此损失函数近似的更加精准。  
3. 对每棵子树增加一个参数，是的每棵子树的权重降低，防止过拟合，这个参数叫shrinkage，对特征进行降采样，灵感来自于随机森林，除了能降低计算量外，还能防止过拟合。  
4. 实现了利用分桶/分位数方法，实现了全局和局部的近似分裂点算法，降低了计算量，并且在eps参数设置合理的情况下，能达到穷举法几乎一样的性能。  
5. 提出并实现了特征带权重的分位数的方法。  
6. 增加处理缺失值的方案（通过枚举所有缺失值在当前节点是进入左子树还是进入右子树更优来决定一个处理缺失值默认的方向）。  

### 系统层面  
7. 对每个特征进行分块（block）并排序，使得在寻找最佳分裂点的时候能够并行化计算。这也是Xgboost比一般GBDT更快的一个重要原因。
8. 通过设置合理的block的大小，充分利用了CPU缓存进行读取加速（cache-aware access）。使得数据读取的速度更快。因为太小的block的尺寸使得多线程中每个线程负载太小，降低了并行效率。太大的block尺寸会导致CPU缓存获取miss掉。  
9. out-of-core 通过将block压缩（block compressoin）并存储到硬盘上，并且通过将block分区到多个硬盘上（block Sharding）实现了更大的IO 读写速度，因此，因为加入了硬盘存储block读写的部分不仅仅使得xgboost处理大数据量的能力有所提升，并且通过提高IO的吞吐量使得xgboost相比一般实利用这种技术实现大数据计算的框架更快。
